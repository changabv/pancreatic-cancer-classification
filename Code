import pandas as pd
import gzip

# Path to the downloaded .gz file
file_path = '/content/GSE28735_series_matrix.txt.gz'

# Open the gzipped file and read it into a DataFrame
with gzip.open(file_path, 'rt') as f:
    df = pd.read_csv(f, sep='\t', comment='!', index_col=0)  # Assuming tab-separated and first column as index

# Inspect the first few rows of the data
df.head()

# Log-transform if the data is counts or has skewed distributions
import numpy as np
df_log = np.log1p(df)  # log(x + 1) transformation

# Handling missing values: e.g., fill with the mean or remove
df_clean = df_log.fillna(df_log.mean())

# Ensure that the columns represent the samples and rows represent genes
print(df_clean.shape)

# Check the first few sample names
for sample in sample_names[:20]:  # Print the first 20 sample names to inspect them
    print(sample)

import pandas as pd

# Example list of GSM identifiers (sample names)
sample_names = [
  #use your sample
]

# Generate labels based on whether the sample number (GSM ID) is odd or even
#This is just an example based on the dataset I used. 
labels = [1 if int(sample[4:]) % 2 == 0 else 0 for sample in sample_names]

# Create a DataFrame with sample names and their corresponding labels
df_labels = pd.DataFrame({'Sample': sample_names, 'Label': labels})

# Print the DataFrame to verify labels
print(df_labels.head())

import pandas as pd
import gzip

# Update this path to the correct location where you uploaded the file
file_path = '/content/GSE28735_series_matrix.txt.gz'

# Load the data (assuming it's gzipped)
with gzip.open(file_path, 'rt') as f:
    df_expression = pd.read_csv(f, sep='\t', comment='!', index_col=0)

# Check the first few rows to verify
print(df_expression.head())

# Generate labels based on whether the sample number is odd (Normal) or even (Tumor)
labels = [1 if int(sample[4:]) % 2 == 0 else 0 for sample in df_expression.columns]

# Create a DataFrame with sample names and their corresponding labels
df_labels = pd.DataFrame({'Sample': df_expression.columns, 'Label': labels})

# Print the first few rows of the label DataFrame to verify
print(df_labels.head())

from sklearn.preprocessing import StandardScaler

# Initialize the scaler
scaler = StandardScaler()

# Apply the scaling to the gene expression data
X_scaled = scaler.fit_transform(df_expression)  # Scale the data

# Verify the shape of the scaled data
print(X_scaled.shape)

# Labels should correspond to the number of samples (columns)
labels = [1 if int(sample[4:]) % 2 == 0 else 0 for sample in df_expression.columns]

# Ensure the labels have the correct length (should be equal to number of samples, 90)
y = np.array(labels)

# Verify the shape of y
print("Shape of y (labels):", y.shape)  # Should be (90,)

print(f"Shape of X_scaled: {X_scaled.shape}")  # Expected: (90, 28869)
print(f"Shape of y (labels): {y.shape}")  # Expected: (90,)

# Transpose the data to have samples as rows and genes as columns
X_scaled = X_scaled.T

# Check the shape again
print(f"Shape of X_scaled after transposing: {X_scaled.shape}")

from sklearn.model_selection import train_test_split

# Split the data into training and testing sets (80% training, 20% testing)
X_train, X_test, y_train, y_test = train_test_split(X_scaled, y, test_size=0.2, random_state=42)

# Check the shapes of the split data
print(f"Training data shape: {X_train.shape}")
print(f"Test data shape: {X_test.shape}")

from sklearn.model_selection import GridSearchCV
from sklearn.ensemble import RandomForestClassifier

# Define a smaller parameter grid for faster search
param_grid = {
    'n_estimators': [50, 100],  # fewer trees for faster search
    'max_depth': [10, 20, None],  # Limit depth of trees for speed
}

# Initialize GridSearchCV with Random Forest classifier
grid_search = GridSearchCV(RandomForestClassifier(random_state=42), param_grid, cv=3, n_jobs=-1)

# Fit the grid search to the data
grid_search.fit(X_train, y_train)

# Get the best parameters from the grid search
print("Best Parameters:", grid_search.best_params_)

# Train the model using the best parameters
best_rf_model = grid_search.best_estimator_

from sklearn.metrics import classification_report, accuracy_score, confusion_matrix

# Make predictions on the test set
y_pred = best_rf_model.predict(X_test)

# Evaluate the accuracy of the model
accuracy = accuracy_score(y_test, y_pred)
print(f"Accuracy: {accuracy:.2f}")

# Classification report to see precision, recall, and F1-score
print("Classification Report:\n", classification_report(y_test, y_pred))

# Confusion Matrix to visualize true vs predicted labels
print("Confusion Matrix:\n", confusion_matrix(y_test, y_pred))

# Get feature importances from the trained Random Forest model
importances = best_rf_model.feature_importances_

# Sort the feature importances in descending order
indices = np.argsort(importances)[::-1]

# Display the top 20 important genes (using row indices)
top_genes = df_expression.index[indices][:20]  # Top 20 genes (row indices)

print("Top 20 important genes:", top_genes)

# Print the top importance values and corresponding gene IDs
print("Top 20 Important Genes (IDs):", top_genes)
print("Top 20 Importances:", top_importances)

import matplotlib.pyplot as plt

# Plotting the bar chart with scaled feature importance values
plt.figure(figsize=(12, 8))
plt.barh(top_gene_ids.astype(str), scaled_importances, color='skyblue')  # Converting IDs to strings for better readability

# Label the axes
plt.xlabel('Feature Importance (Scaled)')
plt.ylabel('Top 20 Important Genes (Gene IDs)')
plt.title('Top 20 Important Genes Based on Feature Importance')

# Invert y-axis to show the most important gene at the top
plt.gca().invert_yaxis()

# Display the plot
plt.tight_layout()  # To prevent clipping of labels
plt.show()

